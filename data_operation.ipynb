{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VNalUZJ-9mz_"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5H5-9Vf-DOz",
        "outputId": "d8095b86-4fcb-4872-9727-3d6152160b9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1.9348e-19, 1.6631e+22, 2.0283e-19],\n",
            "        [1.8990e+28, 1.8177e+31, 1.7418e+28],\n",
            "        [7.1463e+22, 7.6724e+34, 4.4646e+30],\n",
            "        [7.5569e+28, 1.0579e+21, 1.2120e+25],\n",
            "        [7.2444e+22, 6.6532e-33, 1.3563e-19]])\n"
          ]
        }
      ],
      "source": [
        "x = torch.empty(5, 3)\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaBomX52-Mkc",
        "outputId": "d40c7d78-4396-4815-945d-47ac632ba30f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.4810, 0.2170, 0.7266],\n",
            "        [0.7882, 0.6576, 0.7462],\n",
            "        [0.5876, 0.4283, 0.6218],\n",
            "        [0.1446, 0.0647, 0.3460],\n",
            "        [0.9513, 0.4966, 0.7120]])\n"
          ]
        }
      ],
      "source": [
        "x = torch.rand(5, 3)\n",
        "print(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvgOMuYw-VEA",
        "outputId": "eab74e54-d29e-4b73-e589-a3e4379bc65f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0, 0, 0],\n",
            "        [0, 0, 0],\n",
            "        [0, 0, 0],\n",
            "        [0, 0, 0],\n",
            "        [0, 0, 0]])\n"
          ]
        }
      ],
      "source": [
        "x = torch.zeros(5, 3, dtype=torch.long)\n",
        "print(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxgBZN-M-5pW",
        "outputId": "342e6c18-ada7-4130-d3ad-aa1caa6541d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([5.5000, 3.0000])\n"
          ]
        }
      ],
      "source": [
        "x = torch.tensor([5.5, 3])\n",
        "print(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktdhcGtu_04d",
        "outputId": "b8d1e83b-4dcb-4309-f28e-b3adda6643ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]], dtype=torch.float64)\n",
            "tensor([[-0.0254, -1.2514,  0.2989],\n",
            "        [ 3.7436, -0.5737,  0.2813],\n",
            "        [ 0.5093, -0.4422, -0.1360],\n",
            "        [ 1.0209, -1.0003,  0.5632],\n",
            "        [-1.1061,  0.5982,  1.0232]])\n",
            "torch.Size([5, 3])\n",
            "torch.Size([5, 3])\n"
          ]
        }
      ],
      "source": [
        "x = x.new_ones(5, 3, dtype=torch.float64)  # 返回的tensor默认具有相同的torch.dtype和torch.device\n",
        "print(x)\n",
        "\n",
        "x = torch.randn_like(x, dtype=torch.float) # 指定新的数据类型\n",
        "print(x)\n",
        "\n",
        "print(x.size())\n",
        "print(x.shape)\n",
        "\n",
        "# 注意：返回的torch.Size其实就是一个tuple, 支持所有tuple的操作。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-FUw7rNAPhM"
      },
      "source": [
        "还有很多函数可以创建Tensor，去翻翻官方API就知道了，下表给了一些常用的作参考。\n",
        "\n",
        "函数\t           \n",
        "Tensor(*sizes): 基础构造函数\n",
        "\n",
        "tensor(data,): 类似np.array的构造函数\n",
        "\n",
        "ones(*sizes): 全1Tensor\n",
        "\n",
        "\n",
        "zeros(*sizes): 全0Tensor\n",
        "\n",
        "eye(*sizes): 对角线为1，其他为0\n",
        "\n",
        "arange(s,e,step):\t从s到e，步长为step\n",
        "\n",
        "linspace(s,e,steps):\t从s到e，均匀切分成steps份\n",
        "\n",
        "rand/randn(*sizes):\t均匀/标准分布\n",
        "\n",
        "normal(mean,std)/uniform(from,to): 正态分布/均匀分布\n",
        "\n",
        "randperm(m): 随机排列\n",
        "\n",
        "这些创建方法都可以在创建的时候指定数据类型dtype和存放device(cpu/gpu)。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apqiwI-JAnkA",
        "outputId": "6322a876-73e3-44ce-ead3-f2eeb145e049"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[3.1725e+32, 4.5422e-41, 3.1725e+32, 4.5422e-41],\n",
            "        [7.0118e-25, 3.2027e-41, 7.0118e-25, 3.2027e-41]])\n"
          ]
        }
      ],
      "source": [
        "x = torch.Tensor(2,4)\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fofp-ZHplRSL",
        "outputId": "df468599-cbd0-47bd-f4a8-0d9133ab16d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]], dtype=torch.float64)\n",
            "tensor([[1.9134, 1.5444, 1.5629],\n",
            "        [1.4416, 1.1371, 1.4866],\n",
            "        [1.0599, 1.8381, 1.9252],\n",
            "        [1.7050, 1.7274, 1.5683],\n",
            "        [1.3492, 1.6116, 1.9756]], dtype=torch.float64)\n",
            "tensor([[1.9134, 1.5444, 1.5629],\n",
            "        [1.4416, 1.1371, 1.4866],\n",
            "        [1.0599, 1.8381, 1.9252],\n",
            "        [1.7050, 1.7274, 1.5683],\n",
            "        [1.3492, 1.6116, 1.9756]], dtype=torch.float64)\n",
            "tensor([[1.9134, 1.5444, 1.5629],\n",
            "        [1.4416, 1.1371, 1.4866],\n",
            "        [1.0599, 1.8381, 1.9252],\n",
            "        [1.7050, 1.7274, 1.5683],\n",
            "        [1.3492, 1.6116, 1.9756]])\n",
            "tensor([[1.9134, 1.5444, 1.5629],\n",
            "        [1.4416, 1.1371, 1.4866],\n",
            "        [1.0599, 1.8381, 1.9252],\n",
            "        [1.7050, 1.7274, 1.5683],\n",
            "        [1.3492, 1.6116, 1.9756]])\n"
          ]
        }
      ],
      "source": [
        "x = x.new_ones(5, 3, dtype=torch.float64)  # 返回的tensor默认具有相同的torch.dtype和torch.device\n",
        "print(x)\n",
        "\n",
        "# x = torch.randn_like(x, dtype=torch.float) # 指定新的数据类型\n",
        "# print(x)\n",
        "\n",
        "y = torch.rand(5, 3)\n",
        "print(x + y)\n",
        "\n",
        "print(torch.add(x, y))\n",
        "\n",
        "result = torch.empty(5, 3)\n",
        "torch.add(x, y, out=result)\n",
        "print(result)\n",
        "\n",
        "# adds x to y\n",
        "y.add_(x)\n",
        "print(y)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlK5gLPXmjMG",
        "outputId": "d5102c13-51ec-42d5-8ca1-decc91d90a05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([2., 2., 2.], dtype=torch.float64)\n",
            "tensor([2., 2., 2.], dtype=torch.float64)\n"
          ]
        }
      ],
      "source": [
        "# 索引\n",
        "# 我们还可以使用类似NumPy的索引操作来访问Tensor的一部分，需要注意的是：索引出来的结果与原数据共享内存，也即修改一个，另一个会跟着修改。\n",
        "\n",
        "y = x[0, :]\n",
        "y += 1\n",
        "print(y)\n",
        "print(x[0, :]) # 源tensor也被改了\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PADQBdtJngjX",
        "outputId": "28869afb-ad39-483e-bb64-bd02e06a8633"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[4., 4., 4.],\n",
            "        [3., 3., 3.],\n",
            "        [3., 3., 3.],\n",
            "        [3., 3., 3.],\n",
            "        [3., 3., 3.]], dtype=torch.float64)\n",
            "torch.Size([5, 3]) torch.Size([15]) torch.Size([3, 5])\n",
            "tensor([[5., 5., 5.],\n",
            "        [4., 4., 4.],\n",
            "        [4., 4., 4.],\n",
            "        [4., 4., 4.],\n",
            "        [4., 4., 4.]], dtype=torch.float64)\n",
            "tensor([5., 5., 5., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.],\n",
            "       dtype=torch.float64)\n",
            "tensor([[4., 4., 4.],\n",
            "        [3., 3., 3.],\n",
            "        [3., 3., 3.],\n",
            "        [3., 3., 3.],\n",
            "        [3., 3., 3.]], dtype=torch.float64)\n",
            "tensor([5., 5., 5., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.],\n",
            "       dtype=torch.float64)\n",
            "tensor([0.3868])\n",
            "0.3867504596710205\n"
          ]
        }
      ],
      "source": [
        "# 改变形状\n",
        "# 用view()来改变Tensor的形状：\n",
        "print(x)\n",
        "\n",
        "y = x.view(15)\n",
        "z = x.view(-1, 5)  # -1所指的维度可以根据其他维度的值推出来\n",
        "print(x.size(), y.size(), z.size())\n",
        "\n",
        "# 注意view()返回的新Tensor与源Tensor虽然可能有不同的size，但是是共享data的，也即更改其中的一个，另外一个也会跟着改变。(顾名思义，view仅仅是改变了对这个张量的观察角度，内部数据并未改变)\n",
        "x += 1\n",
        "print(x)\n",
        "print(y) # 也加了1\n",
        "\n",
        "# 所以如果我们想返回一个真正新的副本（即不共享data内存）该怎么办呢？Pytorch还提供了一个reshape()可以改变形状，但是此函数并不能保证返回的是其拷贝，所以不推荐使用。推荐先用clone创造一个副本然后再使用view。\n",
        "x_cp = x.clone().view(15)\n",
        "x -= 1\n",
        "print(x)\n",
        "print(x_cp)\n",
        "\n",
        "# 使用clone还有一个好处是会被记录在计算图中，即梯度回传到副本时也会传到源Tensor。\n",
        "\n",
        "# 另外一个常用的函数就是item(), 它可以将一个标量Tensor转换成一个Python number：\n",
        "x = torch.randn(1)\n",
        "print(x)\n",
        "print(x.item())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jg03SpQluP1a"
      },
      "source": [
        "线性代数\n",
        "\n",
        "另外，PyTorch还支持一些线性函数，这里提一下，免得用起来的时候自己造轮子，具体用法参考官方文档。如下表所示：\n",
        "\n",
        "\n",
        "trace\t对角线元素之和(矩阵的迹)\n",
        "\n",
        "diag\t对角线元素\n",
        "triu/tril\t矩阵的上三角/下三角，可指定偏移量\n",
        "\n",
        "mm/bmm\t矩阵乘法，batch的矩阵乘法\n",
        "\n",
        "addmm/addbmm/addmv/addr/baddbmm..\t矩阵运算\n",
        "\n",
        "t\t转置\n",
        "\n",
        "dot/cross\t内积/外积\n",
        "\n",
        "inverse\t求逆矩阵\n",
        "\n",
        "svd\t奇异值分解\n",
        "\n",
        "PyTorch中的Tensor支持超过一百种操作，包括转置、索引、切片、数学运算、线性代数、随机数等等，可参考官方文档。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAAvCI4B8swD",
        "outputId": "190ed045-2118-4919-8762-d7e47cd4e260"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1, 2]])\n",
            "tensor([[1],\n",
            "        [2],\n",
            "        [3]])\n",
            "tensor([[2, 3],\n",
            "        [3, 4],\n",
            "        [4, 5]])\n"
          ]
        }
      ],
      "source": [
        "# 广播机制\n",
        "# 前面我们看到如何对两个形状相同的Tensor做按元素运算。当对两个形状不同的Tensor按元素运算时，可能会触发广播（broadcasting）机制：先适当复制元素使这两个Tensor形状相同后再按元素运算。例如：\n",
        "\n",
        "x = torch.arange(1, 3).view(1, 2)\n",
        "print(x)\n",
        "y = torch.arange(1, 4).view(3, 1)\n",
        "print(y)\n",
        "print(x + y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7KbZQA69uqQ",
        "outputId": "621d2782-a90e-415e-8c8f-669702234422"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "135221121705568\n",
            "tensor([4, 6])\n",
            "135221121705648\n",
            "False\n",
            "tensor([4, 6])\n",
            "True\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "# 运算的内存开销\n",
        "\n",
        "# 前面说了，索引操作是不会开辟新内存的，而像y = x + y这样的运算是会新开内存的，然后将y指向新内存。为了演示这一点，我们可以使用Python自带的id函数：如果两个实例的ID一致，那么它们所对应的内存地址相同；反之则不同。\n",
        "\n",
        "x = torch.tensor([1, 2])\n",
        "y = torch.tensor([3, 4])\n",
        "id_before = id(y)\n",
        "print(id_before)\n",
        "y = y + x\n",
        "print(y)\n",
        "print(id(y))\n",
        "print(id(y) == id_before) # False\n",
        "\n",
        "\n",
        "x = torch.tensor([1, 2])\n",
        "y = torch.tensor([3, 4])\n",
        "id_before = id(y)\n",
        "y[:] = y + x\n",
        "print(y)\n",
        "print(id(y) == id_before) # True\n",
        "\n",
        "# 我们还可以使用运算符全名函数中的out参数或者自加运算符+=(也即add_())达到上述效果，例如torch.add(x, y, out=y)和y += x(y.add_(x))。\n",
        "x = torch.tensor([1, 2])\n",
        "y = torch.tensor([3, 4])\n",
        "id_before = id(y)\n",
        "torch.add(x, y, out=y) # y += x, y.add_(x)\n",
        "print(id(y) == id_before) # True\n",
        "\n",
        "# 注：虽然view返回的Tensor与源Tensor是共享data的，但是依然是一个新的Tensor（因为Tensor除了包含data外还有一些其他属性），二者id（内存地址）并不一致。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSsjX7vTHgHD",
        "outputId": "9f4beb22-ee52-4e1a-e299-a324a987ae0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1., 1., 1., 1., 1.]) [1. 1. 1. 1. 1.]\n",
            "tensor([2., 2., 2., 2., 2.]) [2. 2. 2. 2. 2.]\n",
            "tensor([3., 3., 3., 3., 3.]) [3. 3. 3. 3. 3.]\n",
            "[1. 1. 1. 1. 1.] tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
            "[2. 2. 2. 2. 2.] tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
            "[3. 3. 3. 3. 3.] tensor([3., 3., 3., 3., 3.], dtype=torch.float64)\n",
            "[4. 4. 4. 4. 4.] tensor([3., 3., 3., 3., 3.], dtype=torch.float64)\n"
          ]
        }
      ],
      "source": [
        "# Tensor和NumPy相互转换\n",
        "# 我们很容易用numpy()和from_numpy()将Tensor和NumPy中的数组相互转换。但是需要注意的一点是： 这两个函数所产生的的Tensor和NumPy中的数组共享相同的内存（所以他们之间的转换很快），改变其中一个时另一个也会改变！！！\n",
        "\n",
        "# 还有一个常用的将NumPy中的array转换成Tensor的方法就是torch.tensor(), 需要注意的是，此方法总是会进行数据拷贝（就会消耗更多的时间和空间），所以返回的Tensor和原来的数据不再共享内存。\n",
        "\n",
        "# Tensor转NumPy\n",
        "# 使用numpy()将Tensor转换成NumPy数组:\n",
        "a = torch.ones(5)\n",
        "b = a.numpy()\n",
        "print(a, b)\n",
        "\n",
        "a += 1\n",
        "print(a, b)\n",
        "b += 1\n",
        "print(a, b)\n",
        "\n",
        "# NumPy数组转Tensor\n",
        "# 使用from_numpy()将NumPy数组转换成Tensor:\n",
        "import numpy as np\n",
        "a = np.ones(5)\n",
        "b = torch.from_numpy(a)\n",
        "print(a, b)\n",
        "\n",
        "a += 1\n",
        "print(a, b)\n",
        "b += 1\n",
        "print(a, b)\n",
        "\n",
        "# 所有在CPU上的Tensor（除了CharTensor）都支持与NumPy数组相互转换。\n",
        "\n",
        "# 此外上面提到还有一个常用的方法就是直接用torch.tensor()将NumPy数组转换成Tensor，需要注意的是该方法总是会进行数据拷贝，返回的Tensor和原来的数据不再共享内存。\n",
        "c = torch.tensor(a)\n",
        "a += 1\n",
        "print(a, c)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HXXE3vxKyVp",
        "outputId": "5d01cc9c-8420-4dc6-fbce-da91e0b1b23d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([2, 3], device='cuda:0')\n",
            "tensor([2., 3.], dtype=torch.float64)\n"
          ]
        }
      ],
      "source": [
        "# Tensor on GPU\n",
        "# 用方法to()可以将Tensor在CPU和GPU（需要硬件支持）之间相互移动。\n",
        "# print(x)\n",
        "\n",
        "# 以下代码只有在PyTorch GPU版本上才会执行\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")          # GPU\n",
        "    y = torch.ones_like(x, device=device)  # 直接创建一个在GPU上的Tensor\n",
        "    # print(x)\n",
        "    # print(y)\n",
        "    x = x.to(device)                       # 等价于 .to(\"cuda\")\n",
        "    # print(x)\n",
        "    z = x + y\n",
        "    print(z)\n",
        "    print(z.to(\"cpu\", torch.double))       # to()还可以同时更改数据类型\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
